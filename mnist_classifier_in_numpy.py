# -*- coding: utf-8 -*-
"""MNIST Classifier in NumPy

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xrSYpwdDA1SN11_z1jYZWJX2sAjRcHK8

## Task 1: Load MNIST Dataset
"""

from sklearn.datasets import fetch_openml

mnist = fetch_openml('mnist_784', version=1, as_frame=False)
X = mnist['data']
y = mnist['target'].astype(int)

print('Dataset loaded:', X.shape, y.shape)

"""## Task 2: Display Random Images

"""

import matplotlib.pyplot as plt
import numpy as np

indices = np.random.choice(len(X), 5, replace=False)
plt.figure(figsize=(10, 2))
for i, idx in enumerate(indices):
    plt.subplot(1, 5, i+1)
    plt.imshow(X[idx].reshape(28, 28), cmap='gray')
    plt.title(y[idx])
    plt.axis('off')
plt.show()

"""## Task 3: Normalize Pixel Values"""

X = X / 255.0

"""## Task 4: Split into Training and Testing Sets

"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=10000, random_state=42
)
print('Training set:', X_train.shape, y_train.shape)
print('Test set:', X_test.shape, y_test.shape)

"""## Task 5: Activation Functions & MSE Loss

"""

import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def sigmoid_derivative(a):
    return a * (1 - a)

def softmax(z):
    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)

def mse_loss(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

"""## Task 6: One-Hot Encoding

"""

def one_hot_encode(y, num_classes=10):
    m = len(y)
    one_hot = np.zeros((m, num_classes))
    one_hot[np.arange(m), y] = 1
    return one_hot

y_train_ohe = one_hot_encode(y_train)
y_test_ohe  = one_hot_encode(y_test)
print(y_train_ohe[:5])

"""## Task 7: Initialize Weight Matrices


"""

np.random.seed(42)
W1 = np.random.randn(784, 256)
W2 = np.random.randn(256, 128)
W3 = np.random.randn(128, 64)
W4 = np.random.randn(64, 10)
weights = [W1, W2, W3, W4]

"""## Task 8: Forward Function


"""

def forward(X, weights):
    As = [X]
    Zs = []
    A = X

    for W in weights[:-1]:
        Z = A @ W
        A = sigmoid(Z)
        Zs.append(Z)
        As.append(A)

    Z = A @ weights[-1]
    A = softmax(Z)
    Zs.append(Z)
    As.append(A)
    return Zs, As

"""## Task 9: Backward Function


"""

def backward(Zs, As, y_true, weights):
    m = y_true.shape[0]
    L = len(weights)
    dWs = [None]*L


    dZ = (As[-1] - y_true) / m
    dWs[-1] = As[-2].T @ dZ


    for l in range(L-2, -1, -1):
        dA = dZ @ weights[l+1].T
        dZ = dA * sigmoid_derivative(As[l+1])
        dWs[l] = As[l].T @ dZ

    return dWs

"""## Task 10: Train Function


"""

def train(X, y_true, weights,
          epochs=10, batch_size=64, learning_rate=0.01,
          loss_fn=mse_loss):
    for epoch in range(1, epochs+1):
        perm = np.random.permutation(X.shape[0])
        X_sh, y_sh = X[perm], y_true[perm]
        for i in range(0, X.shape[0], batch_size):
            Xb = X_sh[i:i+batch_size]
            yb = y_sh[i:i+batch_size]
            Zs, As = forward(Xb, weights)
            dWs = backward(Zs, As, yb, weights)
            weights = [W - learning_rate*dW for W, dW in zip(weights, dWs)]

        _, As_full = forward(X, weights)
        loss = loss_fn(y_true, As_full[-1])
        acc = np.mean(np.argmax(As_full[-1], axis=1) == np.argmax(y_true, axis=1))
        print(f"Epoch {epoch}/{epochs} – Loss: {loss:.4f}, Acc: {acc:.4f}")
    return weights

"""## Task 11: Predict Function


"""

def predict(X, weights):
    _, As = forward(X, weights)
    return np.argmax(As[-1], axis=1)

"""## Task 12: Experiments

### Task 12-A: Learning Rate Experiment
"""

learning_rates = [3.0,2.5,0.1]
for lr in learning_rates:
    print(f"Learning Rate = {lr}")
    ws = [np.random.randn(*w.shape) for w in weights]
    ws_trained = train(X_train, y_train_ohe, ws, epochs=5, learning_rate=lr)
    preds = predict(X_test, ws_trained)
    acc = np.mean(preds == y_test)
    print(f"Test Accuracy: {acc:.4f}\n")

"""### Task 12-B: Architecture Experiment"""

architectures = {
    '256-128-64': [784, 256, 128, 64, 10],
    '512-256':   [784, 512, 256, 10],
    '64':        [784, 64, 10]
}

for name, layers in architectures.items():
    print(f"\nArchitecture = {name}")
    ws = [np.random.randn(layers[i], layers[i+1])
          for i in range(len(layers)-1)]
    ws_trained = train(
        X_train, y_train_ohe, ws,
        epochs=5, learning_rate=0.1,
        loss_fn=mse_loss
    )
    preds = predict(X_test, ws_trained)
    acc = np.mean(preds == y_test)
    print(f"Test Accuracy: {acc:.4f}")

"""### Task 12-C: Tanh Activation"""

def tanh(z):
    return np.tanh(z)

def tanh_derivative(a):
    return 1 - np.square(a)

def forward_tanh(X, weights):
    As = [X]
    Zs = []
    A = X

    for W in weights[:-1]:
        Z = A @ W
        A = tanh(Z)
        Zs.append(Z)
        As.append(A)

    Z = A @ weights[-1]
    A = softmax(Z)
    Zs.append(Z)
    As.append(A)

    return Zs, As


def backward_tanh(Zs, As, y_true, weights):
    m = y_true.shape[0]
    L = len(weights)
    dWs = [None]*L

    dZ = (As[-1] - y_true) / m
    dWs[-1] = As[-2].T @ dZ

    for l in range(L-2, -1, -1):
        dA = dZ @ weights[l+1].T
        dZ = dA * tanh_derivative(As[l+1])
        dWs[l] = As[l].T @ dZ

    return dWs


def train_tanh(X, y_true, weights,
               epochs=10, batch_size=64,
               learning_rate=0.01, loss_fn=mse_loss):
    global forward, backward
    orig_fwd, orig_bwd = forward, backward
    forward, backward = forward_tanh, backward_tanh

    trained_weights = train(
        X, y_true, weights,
        epochs=epochs,
        batch_size=batch_size,
        learning_rate=learning_rate,
        loss_fn=loss_fn
    )


    forward, backward = orig_fwd, orig_bwd
    return trained_weights

ws = [np.random.randn(*W.shape) for W in weights]
ws_tanh = train_tanh(X_train, y_train_ohe, ws,
                     epochs=5, learning_rate=0.1)

"""### Task 12-D: Cross-Entropy Loss"""

def forward_ce(X, weights):
    As = [X]
    Zs = []
    A = X


    for W in weights[:-1]:
        Z = A @ W
        A = tanh(Z)
        Zs.append(Z)
        As.append(A)


    Z = A @ weights[-1]
    A = softmax(Z)
    Zs.append(Z)
    As.append(A)

    return Zs, As

def backward_ce(Zs, As, y_true, weights):
    m = y_true.shape[0]
    L = len(weights)
    dWs = [None] * L


    A_out = As[-1]
    dZ = (A_out - y_true) / m
    dWs[-1] = As[-2].T @ dZ


    for l in range(L-2, -1, -1):
        dA = dZ @ weights[l+1].T
        dZ = dA * tanh_derivative(As[l+1])
        dWs[l] = As[l].T @ dZ

    return dWs

def cross_entropy_loss(y_true, y_pred):
    m = y_true.shape[0]
    eps = 1e-12
    return -np.sum(y_true * np.log(y_pred + eps)) / m

def train_ce(X, y_true, weights,epochs=10, batch_size=64, learning_rate=0.01):
    n = X.shape[0]

    for epoch in range(1, epochs+1):
        perm = np.random.permutation(n)
        X_sh, y_sh = X[perm], y_true[perm]

        for i in range(0, n, batch_size):
            Xb = X_sh[i:i+batch_size]
            yb = y_sh[i:i+batch_size]

            Zs, As = forward_ce(Xb, weights)
            dWs     = backward_ce(Zs, As, yb, weights)
            weights = [W - learning_rate*dW for W, dW in zip(weights, dWs)]

        _, As_full = forward_ce(X, weights)
        loss = cross_entropy_loss(y_true, As_full[-1])
        preds = np.argmax(As_full[-1], axis=1)
        acc   = np.mean(preds == np.argmax(y_true, axis=1))

        print(f"Epoch {epoch}/{epochs} – Loss: {loss:.4f}, Acc: {acc:.4f}")

    return weights


def predict_ce(X, weights):
    _, As = forward_ce(X, weights)
    return np.argmax(As[-1], axis=1)


layers = [784, 128, 64, 10]
weights = [np.random.randn(layers[i], layers[i+1]) * 0.01
           for i in range(len(layers)-1)]

ws_trained = train_ce(X_train, y_train_ohe, weights,epochs=5, batch_size=64, learning_rate=0.1)

preds = predict_ce(X_test, ws_trained)
print("Test Accuracy:", np.mean(preds == y_test))

"""### Task 12-E: Simple Architectures"""

simple_archs = {
    '784-10':       [784, 10],
    '784-64-10':    [784, 64, 10],
    '784-128-10':   [784, 128, 10]
}

for name, layers in simple_archs.items():
    print(f"Architecture = {name}")
    ws = [np.random.randn(layers[i], layers[i+1]) for i in range(len(layers)-1)]
    ws_trained = train(X_train, y_train_ohe, ws, epochs=5, learning_rate=0.01)
    preds = predict(X_test, ws_trained)
    acc = np.mean(preds == y_test)
    print(f"Test Accuracy: {acc:.4f}\n")